{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd93eaa-4db7-420a-983c-4e81acbea260",
   "metadata": {},
   "source": [
    "# Location-based Text Mining\n",
    "\n",
    "**In this notebook an approach to identify documents related to LU from large historical archives, without ex-ante knowledge of the content of these archives. The notebook is part of a set of three notebooks which can all be found on [GitHub](https://github.com/Yegberink/VOC_land_use).**\n",
    "\n",
    "## Load data\n",
    "\n",
    "The data for the approach is loaded and cleaned in this codeblock using the index of the historical archives. In this research the focus is on Ceylon, however update the label list according to your region or archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114b8a64-d78e-4f07-b619-00fa9ebd2c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install packages if necessary\n",
    "#pip install pandas flair fuzzywuzzy python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2697348d-406b-4648-9282-7287b7d1a17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages and data loaded\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import os\n",
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Set the directory (change accordingly\n",
    "path = \"/Users/Yannick/Documents/RA work\"\n",
    "os.chdir(path)\n",
    "\n",
    "# Load data\n",
    "files_df = pd.read_csv(\"Data/files.csv\") #Text files\n",
    "ceylon_labels = pd.read_table(\"Data/index_ceylon.txt\", header=None) #Labels for ceylon documents\n",
    "\n",
    "#\n",
    "placenames_ceylon = pd.read_excel(\"Data/placenames_sri.xlsx\", sheet_name=\"alloetkoer_corle\", header=None) #placenames in ceylon\n",
    "\n",
    "#filter for the labels of ceylon\n",
    "#Create set of complete labels\n",
    "ceylon_labels_set = set(\"HaNA_1.04.02_\" + ceylon_labels[0].astype(str))\n",
    "\n",
    "#Create short number in the files used for filtering\n",
    "files_df['file_number_short'] = files_df['LabelIdentifier'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "\n",
    "#Filter for the documents that concern Ceylon\n",
    "ceylon_documents = files_df[files_df[\"file_number_short\"].isin(ceylon_labels_set)].reset_index(drop=True)\n",
    "\n",
    "#Make smaller one with just the label and the content\n",
    "ceylon_documents_clean = ceylon_documents[[\"LabelIdentifier\", \"DocumentContent\"]]\n",
    "\n",
    "print(\"packages and data loaded\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb85698-9721-427d-a560-07e1473048a6",
   "metadata": {},
   "source": [
    "## Apply Named Entity Recognition (NER)\n",
    "\n",
    "In this codeblock NER is applpied to find locations in the texts. This runs for a long time (in the case of Ceylon Â±48 hours) and therefore the dataframe is saved after the identification. IMPORTANT: update the save path before you run the codeblock otherwise you have to start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c4cb2-0184-473f-ae84-2448f693020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the NER model\n",
    "tagger = Classifier.load(\"nl-ner-large\")\n",
    "\n",
    "# apply the model to the generale missiven\n",
    "for i in range(len(ceylon_documents_clean)):\n",
    "    \n",
    "    # Extract the question from the second column\n",
    "    content = ceylon_documents_clean.iloc[i, 1]\n",
    "    \n",
    "    # make sentence (tokenisation) --> other tokenisers can be used this one just gives back the words\n",
    "    sentence = Sentence(content)\n",
    "    \n",
    "    # predict NER tags\n",
    "    tagger.predict(sentence)\n",
    "    \n",
    "    #extract the named entitiese \n",
    "    entities = sentence.get_spans('ner')\n",
    "    \n",
    "    # Initiate empty list\n",
    "    location_words = []\n",
    "\n",
    "    #Filter for locations  \n",
    "    for span in entities:\n",
    "        if span.tag == \"LOC\":\n",
    "            location_words.append(span.text) #Append the list\n",
    "            \n",
    "    # Save the list of location words as a string in the dataframe\n",
    "    locations_string = ', '.join(location_words)\n",
    "    ceylon_documents_clean.at[i, \"NamedEntities\"] = locations_string #Save data\n",
    "    \n",
    "    #Track the progress (make the division number smaller to get more feedback\n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "        \n",
    "#%% Save the results (update the path according to your data structure)\n",
    "results_NER_ceylon = ceylon_documents_clean\n",
    "results_NER_ceylon.to_csv(\"output/datasets/NER_Ceylon.csv\", index=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b490a34-cbc0-4f95-aaee-85c26330de8a",
   "metadata": {},
   "source": [
    "## Clean output of NER and split into sentences\n",
    "\n",
    "The output of the NER is now cleaned and sentences of 30 words around the location are created so that these can be used in identifying useful documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78c746d8-fd8a-4b92-ad99-42730145e975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30000\n",
      "60000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Load the NER sentences if needed\n",
    "results_NER_ceylon = pd.read_csv(\"output/datasets/NER_Ceylon.csv\")\n",
    "\n",
    "# define functions\n",
    "# Function to split values at commas and return a single list of named entities\n",
    "def split_and_concatenate(row):\n",
    "    if isinstance(row, str):\n",
    "        entities = row.split(\", \")  # Split the string by comma and space\n",
    "        return entities\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Define function for extracting locations\n",
    "def extract_locations(NER_output):\n",
    "    #Loop over the locations to find locations with at least 2 characters\n",
    "    NER_output_filtered = [loc for loc in NER_output if len(loc) > 2]\n",
    "    locations_from_function = set(NER_output_filtered)\n",
    "    return locations_from_function\n",
    "\n",
    "# split the NER results into lists\n",
    "results_NER_ceylon['listed_values'] = results_NER_ceylon['NamedEntities'].apply(lambda x: split_and_concatenate(x))\n",
    "\n",
    "#Initiate empty list\n",
    "tokened_sentences = []\n",
    "\n",
    "#Extract the locations and save the 30 tokens around it\n",
    "\n",
    "for index, row in results_NER_ceylon.iterrows():\n",
    "    text = row['DocumentContent']  # Extract the content\n",
    "    NER_output = row['listed_values']  # Extract locations\n",
    "    locations_set = extract_locations(NER_output) #clean up the locations\n",
    "\n",
    "    #Tokenise the sentence again using the tokeniser\n",
    "    tokenized_text = Sentence(text)\n",
    "\n",
    "    #Define the context window (Change if you want longer sentences)\n",
    "    context_window = 30\n",
    "\n",
    "    #Loop over tokens from the tokenized text\n",
    "    for i, token in enumerate(tokenized_text):\n",
    "\n",
    "        #Append the list if there is a location in the sentence\n",
    "        if token.text in locations_set:\n",
    "            start_index = max(0, i - context_window)\n",
    "            end_index = min(len(tokenized_text), i + context_window + 1)\n",
    "            context_tokens = tokenized_text[start_index:end_index]\n",
    "            context_text = ', '.join(token.text for token in context_tokens if token.text.strip(\" \") != \",\")\n",
    "            tokened_sentences.append({'LabelIdentifier': row['LabelIdentifier'], 'sentence': context_text, 'location': token.text})\n",
    "    \n",
    "    #Track the progress       \n",
    "    if index % 30000 == 0:\n",
    "        print(index)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "tokened_sentences = pd.DataFrame(tokened_sentences)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d8365-6940-4897-8a83-b417b06dc4ed",
   "metadata": {},
   "source": [
    "## Identify promising pages\n",
    "\n",
    "The last step is to identify promising text based on the locations found in the NER step and a list of known locations. By matching these two lists we can identify documents that metion locations that are probably interesting for LU. This is done using two functions specified below. The first is a sliding window function which is used to break up placenames to ensure that if there are two placenames that are pasted together, these are not ignored. The second is a fuzzy matching function which uses the Levenshtein distance between to strings to compute a similarity score. By assigning a certain threshold only the most alike locations are returned giving a manageable dataframe to assess on the occurence of LU information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98450d8e-27c6-47ce-8f03-4ec4acc47d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate sliding window function\n",
    "def sliding_window(elements, window_size):\n",
    "    \"\"\"\n",
    "    Extracts sliding windows of a specified size from a list.\n",
    "    \n",
    "    Parameters:\n",
    "    elements (list): List of elements.\n",
    "    window_size (int): Size of the sliding window.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of sliding windows.\n",
    "    \"\"\"\n",
    "    if len(elements) <= window_size:\n",
    "        return elements\n",
    "    return [elements[i:i+window_size] for i in range(len(elements)- window_size + 1)]\n",
    "\n",
    "def find_matches(word, list_to_match, threshold=90, threshold2=90):\n",
    "    \"\"\"\n",
    "    Finds matches to a word by fuzzy matching with a list of items.\n",
    "    \n",
    "    Parameters:\n",
    "    word (str): Input word to match.\n",
    "    list_to_match (list): List of items for matching.\n",
    "    threshold (int, optional): Minimum threshold for fuzzy matching. Defaults to 80.\n",
    "    threshold2 (int, optional): Higher threshold for more exact matching used in combination with the sliding window. Defaults to 90.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of tuples containing (word, item, similarity_score).\n",
    "    \"\"\"\n",
    "    matches = []  # Create empty list\n",
    "    \n",
    "    # Iterate over the tokens in the list\n",
    "    for item in list_to_match:  # Iterate over the list \n",
    "        similarity_score = fuzz.ratio(word.lower(), item.lower())  # Apply fuzzy matching to the item in the list and the word that has to be matched\n",
    "        if similarity_score >= threshold:\n",
    "            matches.append((word, item, similarity_score))  # Append the list\n",
    "        \n",
    "            # If the commodity does not represent the exact word there might be a window in which it is correct\n",
    "        else:\n",
    "            for element in sliding_window(word, len(item)): #Loop over the elements of the sliding window\n",
    "                similarity_score = fuzz.ratio(element.lower(), word.lower()) \n",
    "                if similarity_score >= threshold2:\n",
    "                    matches.append((word, item, similarity_score))  # Append the list\n",
    "    \n",
    "    return matches  # Return the list of matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e48777-fb65-43f7-9a1e-117caac81ba1",
   "metadata": {},
   "source": [
    "## Apply functions\n",
    "\n",
    "In the following part the functions are applied and a dataframe of promising text is created by merging with the original documents. An area list is specified to extract only the areas found on the map. The results are saved to be qualitatively assessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e42786a-396c-4d7b-8341-212a613ab2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alloetkoer corle\n",
      "0\n",
      "30000\n",
      "60000\n",
      "90000\n",
      "hina corle\n",
      "0\n",
      "30000\n",
      "60000\n",
      "90000\n",
      "happitigam corle\n",
      "0\n",
      "30000\n",
      "60000\n",
      "90000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Initiate list for promising sentences\n",
    "promising_sentences = []\n",
    "\n",
    "#List of areas\n",
    "areas_list = [\"alloetkoer corle\", \"hina corle\", \"happitigam corle\"]\n",
    "\n",
    "#Iterate over the areas so the placenames can be identified\n",
    "for area in areas_list:\n",
    "    \n",
    "    #Print the area for tracking progress\n",
    "    print(area)\n",
    "    \n",
    "    #filter out locations that give problems with matching\n",
    "    problematic_locations = [\"oeddetoetterepittige\", \"talloewatteheenpittie\"]\n",
    "\n",
    "    # Filter out problematic locations\n",
    "    filtered_placenames = placenames_ceylon.loc[~placenames_ceylon[0].isin(problematic_locations)]\n",
    "    \n",
    "    # Filter the filtered placenames to only have the current area\n",
    "    placename_set = set(filtered_placenames.loc[filtered_placenames[1] == area, 0])\n",
    "        \n",
    "    #Iterate over the tokened_sentences df\n",
    "    for index, row in tokened_sentences.iterrows():\n",
    "        \n",
    "        #Extract the location\n",
    "        location = row[\"location\"]\n",
    "    \n",
    "        #Apply the find_matches function to match the location on the map with a location mentioned in the texts\n",
    "        found_match = find_matches(location, placename_set, threshold=95, threshold2=98)\n",
    "            \n",
    "        #If a match is found add the match and the area to the row and append dataframe\n",
    "        if found_match:\n",
    "            row[\"match\"] = found_match #Add match\n",
    "            row[\"area\"] = area #Add area\n",
    "            promising_sentences.append(row) #append df\n",
    "            \n",
    "        #Print index to track progress\n",
    "        if index % 30000 == 0:\n",
    "            print(index)\n",
    "      \n",
    "#Create df from the list\n",
    "promising_sentences = pd.DataFrame(promising_sentences)\n",
    "\n",
    "#merge the df with the generale missiven\n",
    "#remove the dot from the generale missiven label\n",
    "ceylon_documents[\"labels\"] = ceylon_documents[\"LabelIdentifier\"].str.rstrip(\".\")\n",
    "\n",
    "# Remove trailing dot from the \"LabelIdentifier\" column\n",
    "promising_sentences[\"labels\"] = promising_sentences[\"LabelIdentifier\"].str.rstrip(\".\")\n",
    "\n",
    "#Drop the unnecessary columns\n",
    "ceylon_documents = ceylon_documents.drop('LabelIdentifier', axis=1)\n",
    "\n",
    "#%% merge the df\n",
    "promising_text_ceylon = pd.merge(promising_sentences, ceylon_documents, on=\"labels\", how=\"inner\")\n",
    "promising_text_ceylon = promising_text_ceylon.sort_values(by='labels').reset_index(drop=True)\n",
    "\n",
    "promising_text_ceylon = promising_text_ceylon[[\"labels\", \"DocumentContent\", \"location\", \"area\", \"match\"]]\n",
    "\n",
    "#%% wrtie csv for checking\n",
    "promising_text_ceylon.to_csv(\"output/datasets/promising_text_ceylon.csv\", index=False, sep=';')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bada24-6560-4e9d-a363-e3c43f08524f",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "After the qualitative assessment keywords can be identified and the structure of the documents can be used to inform a second round of text mining, which can be found in the Jupyter notebook keyword_based_TM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
