{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbe511c-027a-467f-b61c-2e7a0e834d99",
   "metadata": {},
   "source": [
    "# Keyword-based Text Mining\n",
    "\n",
    "**In this notebook an approach to identify documents related to LU from large historical archives, with the use of keywords identified based on an earlier round of text mining. The notebook is part of a set of three notebooks which can all be found on [GitHub](https://github.com/Yegberink/VOC_land_use).**\n",
    "\n",
    "## Identify keywords\n",
    "\n",
    "The first step in this process is to identify keywords. This is done by hand, however the following code can help create some insights into what words are actually interesting by looking at the most occuring words in the interesting documents identified in the Location-Based text mining. When more interesting documents are found, this codeblock can be run again to identify additional keywords. To properly run the code, your documents should be in a similar format as datasets.xlsx. You can find the dataset on [GitHub](https://github.com/Yegberink/VOC_land_use) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b14ae7a1-8af6-4a2c-a72a-5216bf2d73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas flair fuzzywuzzy python-Levenshtein bertopic ipywidgets spacy xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28073eb-28da-4525-a5e7-b76fc12f3b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import os\n",
    "from flair.data import Sentence\n",
    "from collections import Counter\n",
    "\n",
    "# Set the directory\n",
    "path = \"/Users/Yannick/Documents/RA work\"\n",
    "os.chdir(path)\n",
    "\n",
    "# Load document numbers of previously assessed documents\n",
    "found_datasets = pd.read_excel(\"output/results/datasets.xlsx\", sheet_name='short_summary_of_datasets')\n",
    "\n",
    "# Load the document content of the documents from Ceylon\n",
    "ceylon_documents_clean = pd.read_csv(\"Data/ceylon_documents_clean\", sep=\";\")\n",
    "\n",
    "# Load stopwords\n",
    "stopwords_nl = pd.read_table('Data/stopwords_nl.txt', header=None)\n",
    "stopwords_nl = stopwords_nl[0].tolist()\n",
    "\n",
    "# initiate empty list to hold all documents that are previously assessed\n",
    "checked_document_labels = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in found_datasets.iterrows():\n",
    "    \n",
    "    #extract useful information\n",
    "    label_number = row['label_number']\n",
    "    start_page = row['start_page']\n",
    "    end_page = row['end_page']\n",
    "    \n",
    "    # Generate labels for each page in the range\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        formatted_page = str(page).zfill(4) #add 0's in front of the page number\n",
    "        label = f\"HaNA_1.04.02_{label_number}_{formatted_page}\" #fix the formatting\n",
    "        checked_document_labels.append(label) #store the labels\n",
    "\n",
    "#initiate empty list to hold all documents that were assessed to be interesting\n",
    "interesting_documents = []\n",
    "\n",
    "#filter for interesting datasets\n",
    "interesting_datasets = found_datasets[(found_datasets['interesting'] == 'yes') | (found_datasets['interesting'] == 'maybe')]\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in interesting_datasets.iterrows():\n",
    "    \n",
    "    #extract useful information\n",
    "    label_number = row['label_number']\n",
    "    start_page = row['start_page']\n",
    "    end_page = row['end_page']\n",
    "    \n",
    "    # Generate labels for each page in the range\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        formatted_page = str(page).zfill(4) #add 0's in front of the page number\n",
    "        label = f\"HaNA_1.04.02_{label_number}_{formatted_page}\" #fix the formatting\n",
    "        interesting_documents.append({\n",
    "            'label': label,\n",
    "            'interesting': row['interesting'],\n",
    "            'year': row['year'],\n",
    "            'list': row['List?'],\n",
    "            'remarks': row['remarks'],\n",
    "            'archive_number': row['label_number']}) \n",
    "\n",
    "\n",
    "interesting_documents = pd.DataFrame(interesting_documents)\n",
    "interesting_documents_labels = set(interesting_documents['label'])\n",
    "\n",
    "# create a df with interesting documents\n",
    "merged_interesting_documents = pd.merge(interesting_documents, ceylon_documents_clean, left_on='label', right_on='LabelIdentifier')\n",
    "\n",
    "# Drop the 'LabelIdentifier' column\n",
    "merged_interesting_documents = merged_interesting_documents.drop('LabelIdentifier', axis=1)\n",
    "\n",
    "# List to hold all tokens\n",
    "all_tokens = []\n",
    "texts = []\n",
    "\n",
    "# Tokenize each document using the flair Sentence tokeniser\n",
    "for text in merged_interesting_documents['DocumentContent']:\n",
    "    sentence = Sentence(text)\n",
    "    tokens = [token.text.lower() for token in sentence.tokens if len(token.text) >= 3 and token.text and token.text.lower() not in stopwords_nl]\n",
    "    all_tokens.extend(tokens)\n",
    "    texts.append(tokens)\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# Display the most common words\n",
    "most_common_words = word_counts.most_common()\n",
    "\n",
    "# Convert the result to a DataFrame for easier viewing\n",
    "most_common_words_df = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "# Save the df\n",
    "most_common_words_df.to_csv(\"output/datasets/keyword_identification.csv\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac4eee-4b61-4c5a-af82-c2f879f71ed7",
   "metadata": {},
   "source": [
    "## BERTopic\n",
    "\n",
    "In the following codeblock a BERTopic modeling is performed to check the results of the keyword identification. In this codeblock a representation of the BERTopic modeling is presented and in the codeblock after this all identified keywords for an interesting topic are extracted. The topic modeling takes some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96bb4a82-1aa5-4053-bd0b-81bd9f310728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-03 11:46:50,902 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches:  15%|████▊                            | 330/2267 [01:38<09:38,  3.35it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 48\u001b[0m\n\u001b[1;32m     36\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m BERTopic(\n\u001b[1;32m     37\u001b[0m     embedding_model \u001b[38;5;241m=\u001b[39m embed_model,\n\u001b[1;32m     38\u001b[0m     umap_model \u001b[38;5;241m=\u001b[39m umap_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# this options ensures that the function returns some info while running\u001b[39;00m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#Apply the model\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m visualisation\u001b[38;5;241m=\u001b[39mtopic_model\u001b[38;5;241m.\u001b[39mvisualize_hierarchy(hierarchical_topics\u001b[38;5;241m=\u001b[39mmy_hier_topic_model)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Save the visualization as an HTML file\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/bertopic/_bertopic.py:431\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[0;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[1;32m    429\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding - Transforming documents to embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m select_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 431\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding - Completed \u001b[39m\u001b[38;5;130;01m\\u2713\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/bertopic/_bertopic.py:3676\u001b[0m, in \u001b[0;36mBERTopic._extract_embeddings\u001b[0;34m(self, documents, images, method, verbose)\u001b[0m\n\u001b[1;32m   3674\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39membed_words(words\u001b[38;5;241m=\u001b[39mdocuments, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m   3675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 3676\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m documents[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3678\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3679\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure to use an embedding model that can either embed documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3680\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor images depending on which you want to embed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3681\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/bertopic/backend/_base.py:62\u001b[0m, in \u001b[0;36mBaseEmbedder.embed_documents\u001b[0;34m(self, document, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, document: List[\u001b[38;5;28mstr\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of n words into an n-dimensional\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    matrix of embeddings.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/bertopic/backend/_sentencetransformers.py:65\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[0;34m(self, documents, verbose)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, documents: List[\u001b[38;5;28mstr\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    matrix of embeddings.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:630\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 630\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    634\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "import spacy\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Extract document content from the previously loaded ceylon_documents_clean\n",
    "data = ceylon_documents_clean['DocumentContent']\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# Transform the dataframe into a list:\n",
    "data = data.apply(lambda row: ' '.join(row.values.astype(str)), axis=1).tolist()\n",
    "\n",
    "# Create the embeddings\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-V2\")\n",
    "\n",
    "# Create the dimension reduction model\n",
    "umap_model = UMAP(n_neighbors = 30, n_components = 5, min_dist = 0.0, metric = \"cosine\", random_state = 42)\n",
    "\n",
    "# Create the clustering model\n",
    "hdbscan_model = HDBSCAN(min_cluster_size = 7, metric = \"euclidean\", cluster_selection_method = 'eom', prediction_data = True)\n",
    "\n",
    "# Create the representation model\n",
    "vect_model = CountVectorizer(stop_words = stopwords_nl, min_df = 2, ngram_range = (1,1)) \n",
    "keybert_model = KeyBERTInspired()\n",
    "mmr_model = MaximalMarginalRelevance(diversity = 0.3)\n",
    "representation_model = {\n",
    "    \"keyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    " #  \"POS\": pos_model\n",
    "}\n",
    "\n",
    "# regroup all the models into one\n",
    "topic_model = BERTopic(\n",
    "    embedding_model = embed_model,\n",
    "    umap_model = umap_model,\n",
    "    hdbscan_model = hdbscan_model,\n",
    "    vectorizer_model = vect_model,\n",
    "    representation_model = representation_model,\n",
    "    top_n_words = 20, # how many words to include in the representation\n",
    "    verbose = True # this options ensures that the function returns some info while running\n",
    ")\n",
    "\n",
    "\n",
    "#Apply the model\n",
    "topics, probs = topic_model.fit_transform(data)\n",
    "\n",
    "visualisation=topic_model.visualize_hierarchy(hierarchical_topics=my_hier_topic_model)\n",
    "\n",
    "# Save the visualization as an HTML file\n",
    "visualisation.write_html(\"hierarchy_visualization.html\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341b0a9-bc94-4056-949a-f50cbc2d1100",
   "metadata": {},
   "source": [
    "## Extract keywords of interesting topics\n",
    "\n",
    "The topics in the hierarchy are analysed manually and the interesting topic numbers can be collected in the interesting_topics list. This is then used to extract the interesting keywords. Any additional interesting keywords should be added to the more_keywords document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accda7d3-6c46-4b38-896a-4a6feeaafac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of interesting topic IDs\n",
    "interesting_topics = [236, 63, 167, 285, 66, 69, 24, 129, 162, 26]\n",
    "\n",
    "# Initialize an empty list to store topic keywords\n",
    "topic_keywords = []\n",
    "\n",
    "# Loop through each interesting topic and extract keywords\n",
    "for topic in interesting_topics:\n",
    "    # Get the keywords and their respective scores for the topic\n",
    "    words, scores = zip(*topic_model.get_topic(topic))\n",
    "    \n",
    "    # Create a dictionary for the topic with words and scores\n",
    "    topic_data = {\n",
    "        \"Topic\": topic,\n",
    "        \"Keywords\": ', '.join(words),  # Join keywords into a single string for readability\n",
    "        \"Scores\": ', '.join([f'{score:.4f}' for score in scores])  # Format the scores\n",
    "    }\n",
    "    \n",
    "    # Append to the list\n",
    "    topic_keywords.append(topic_data)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_topics = pd.DataFrame(topic_keywords)\n",
    "\n",
    "# Save the results\n",
    "df_topics.to_csv(\"output/datasets/BERTopic_keywords.csv\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e7167-fc66-4d7e-80e6-003f4fe19d25",
   "metadata": {},
   "source": [
    "## Load and clean documents for keyword based text mining\n",
    "\n",
    "**The following part of the code can be used independently of the keyword identification. Therefore, some part of the previous code are repeated.**\n",
    "\n",
    "In the following codeblock packages and data are loaded and prepared for the matching of keywords with words in texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a675a06-28e3-4193-80d2-5204c2e3b9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "from flair.data import Sentence\n",
    "from collections import Counter\n",
    "import string\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Set the directory\n",
    "path = \"/Users/Yannick/Documents/RA work\"\n",
    "os.chdir(path)\n",
    "\n",
    "# Load previously assessed documents\n",
    "found_datasets = pd.read_excel(\"output/results/datasets.xlsx\", sheet_name='short_summary_of_datasets')\n",
    "\n",
    "#initiate empty df\n",
    "checked_document_labels = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in found_datasets.iterrows():\n",
    "    \n",
    "    #extract useful information\n",
    "    label_number = row['label_number']\n",
    "    start_page = row['start_page']\n",
    "    end_page = row['end_page']\n",
    "    \n",
    "    # Generate labels for each page in the range\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        formatted_page = str(page).zfill(4) #add 0's in front of the page number\n",
    "        label = f\"HaNA_1.04.02_{label_number}_{formatted_page}\" #fix the formatting\n",
    "        checked_document_labels.append(label) #store the labels\n",
    "\n",
    "# Load ceylon documents\n",
    "ceylon_documents_clean = pd.read_csv(\"Data/ceylon_documents_clean\", sep=\";\")\n",
    "\n",
    "# Load the more_keywords document\n",
    "\n",
    "more_keywords = pd.read_table(\"Data/more_keywords.txt\", header=None) \n",
    "more_keywords = set(more_keywords[0])\n",
    "\n",
    "# List to hold all tokens\n",
    "all_tokens = []\n",
    "texts = []\n",
    "\n",
    "#load stopwords_nl\n",
    "stopwords_nl = pd.read_table('Data/stopwords_nl.txt', header=None)\n",
    "stopwords_nl = stopwords_nl[0].tolist()\n",
    "\n",
    "#Load trade data\n",
    "#Trade datasets\n",
    "cargo_1 = pd.read_excel(\"Data/trade_data/cargos (1).xls\", \"cargo\", header=None)\n",
    "cargo_2 = pd.read_excel(\"Data/trade_data/cargos (2).xls\", \"cargo\", header=None)\n",
    "cargo_3 = pd.read_excel(\"Data/trade_data/cargos (3).xls\", \"cargo\", header=None)\n",
    "\n",
    "#Concat into single df\n",
    "trade_data = pd.concat([cargo_1, cargo_2, cargo_3])\n",
    "\n",
    "#Select first 8 columns\n",
    "trade_data = trade_data.iloc[:,0:8]\n",
    "\n",
    "#Name these columns\n",
    "trade_data.columns = [\"book_year\", \"quantity\", \"unit\", \"product\", \"departure_place\", \"departure_region\", \"arrival_place\", \"arrival_region\"]\n",
    "\n",
    "#Convert - to NAN\n",
    "trade_data.replace('-', None, inplace=True)\n",
    "\n",
    "#Create set of unique commodities\n",
    "commodity_list = set(trade_data['product'])\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bae2a5-56a8-49dd-a1cb-37d0b95ba715",
   "metadata": {},
   "source": [
    "## Flag interesting documents\n",
    "\n",
    "In the following codeblock two keyword based filterings are applied. Firstly, only texts containing a high number of LU keywords are kept(x), and seconly only documents containing a low number of commodities are kept (y). The filtering showed no new documents when in the (x >= 2 and y < 3) or (x >= 3) configuration. The best working filtering was therefore found to be:(x >= 3 and y < 3) or (x >= 4). y was kept stable for the filtering since changing this led to the inclusion of many wrong texts. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d95c61e-814a-4db5-88ef-d3fef5569dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#%%Flag interesting documents\n",
    "\n",
    "flagged_text = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in ceylon_documents_clean.iterrows():\n",
    "    text = row['DocumentContent']  # Extract the document content\n",
    "    tokenized_text = Sentence(text)  # Tokenize the text (assuming Sentence is a tokenizer)\n",
    "    \n",
    "    #initiate counter\n",
    "    x = 0\n",
    "    y = 0\n",
    "   \n",
    "    #initiate empty lists for matched words\n",
    "    found_tokens = []\n",
    "    found_commodities = []\n",
    "    \n",
    "    # Iterate over each token in the tokenized text\n",
    "    for token in tokenized_text:\n",
    "        word = token.text.lower()  # Convert token to lowercase\n",
    "        if word in more_keywords:  # Check if token is in cultivation_keywords\n",
    "            x += 1  # Increment the counter if a keyword is found\n",
    "            found_tokens.append(word)  # Store the matching token\n",
    "        if word in commodity_list:  # Check if token is in commodity_list\n",
    "            y += 1  # Increment the counter if a commodity is found\n",
    "            found_commodities.append(word)  # Store the matching commodity\n",
    "\n",
    "    # Determine the flag and append the result to promising_text\n",
    "    if (x >= 3 and y < 3) or (x >= 4):\n",
    "        flag = 'yes'\n",
    "    else:\n",
    "        flag = 'no'\n",
    "    \n",
    "    #append the list\n",
    "    flagged_text.append({\n",
    "        'LabelIdentifier': row['LabelIdentifier'],  # Store the label identifier\n",
    "        'DocumentContent': text,  # Store the original document content\n",
    "        'tokened_text': tokenized_text,  # Store the tokenized text\n",
    "        'cultivation_words': found_tokens,  # Store the list of found cultivation keywords\n",
    "        'flag': flag,  # Store the determined flag\n",
    "        'commodities': found_commodities  # Store the list of found commodities\n",
    "    })\n",
    "    #Print index to track progress\n",
    "    if index % 10000 == 0:\n",
    "        print(index)\n",
    "\n",
    "#convert to df\n",
    "flagged_text = pd.DataFrame(flagged_text)\n",
    "\n",
    "#%%Filter for documents that have yes or maybe\n",
    "\n",
    "filtered_flagged = flagged_text[(flagged_text['flag'] == 'yes')].reset_index()\n",
    "\n",
    "#%% save the dataframe for checking\n",
    "filtered_flagged.to_csv(\"output/datasets/filtered_flagged_text.csv\", index=False, sep=';')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fef2aa-3aa7-4e6b-aa6c-80bab956fb72",
   "metadata": {},
   "source": [
    "## Table identification\n",
    "\n",
    "The last step in the identification of potentially interesting documents is the identification of tables. Due to the structure of the HTR texts from the GLOBALISED project which is used for this study, this can be done based on punctuation percentage. If the punctuation percentage is lower than 30% we found that many none-tables were returned and therefore the threshold was set on 30%. The resulting dataset is saved and can be qualitatively assessed to find pages on LU. If this assessment is done using the Excel file on [GitHub](https://github.com/Yegberink/VOC_land_use), previously assessed documents are filtered out to make the iterative process of setting the thresholds easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55c21cb9-9aa2-4efb-8aa7-898809afa901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#%% Find documents with high amounts of punctuation\n",
    "\n",
    "#Define functions\n",
    "def sliding_window(elements, window_size):\n",
    "    \"\"\"\n",
    "    Extracts sliding windows of a specified size from a list.\n",
    "    \n",
    "    Parameters:\n",
    "    elements (list): List of elements.\n",
    "    window_size (int): Size of the sliding window.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of sliding windows.\n",
    "    \"\"\"\n",
    "    if len(elements) <= window_size:\n",
    "        return [elements]  # Return the whole list as one window\n",
    "    return [elements[i:i + window_size] for i in range(len(elements) - window_size + 1)]\n",
    "\n",
    "\n",
    "def count_punctuation(sentence):\n",
    "    \"\"\"\n",
    "    Counts the number of punctuation marks in a sentence.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (str): The input sentence.\n",
    "\n",
    "    Returns:\n",
    "    int: The count of punctuation marks in the sentence.\n",
    "    \"\"\"\n",
    "    return sum(1 for char in sentence if char in string.punctuation)\n",
    "\n",
    "\n",
    "def punctuation_percentage(sentence):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of punctuation marks in a sentence. Uses the count_punctuation function.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (str): The input sentence.\n",
    "\n",
    "    Returns:\n",
    "    float: The percentage of punctuation marks in the sentence.\n",
    "    \"\"\"\n",
    "    num_punctuation = count_punctuation(sentence)\n",
    "    total_chars = len(sentence)\n",
    "    return (num_punctuation / total_chars) * 100 if total_chars > 0 else 0\n",
    "\n",
    "#do the filtering \n",
    "\n",
    "promising_text = [] #initiate list\n",
    "\n",
    "#loop ocer the filtered and flagged documents\n",
    "for index, row in filtered_flagged.iterrows():\n",
    "    text = row['tokened_text']  # Extract the document content\n",
    "    \n",
    "    #divide into parts of the document\n",
    "    for element in sliding_window(text, 30):\n",
    "        sentence = \" \".join([str(token.text) for token in element]) #create a workable string\n",
    "        percentage = punctuation_percentage(sentence) #calculate punctuation percentage\n",
    "        \n",
    "        #append the row if there is high punctuation somewhere in the document\n",
    "        if percentage >= 30:\n",
    "            promising_text.append(row)  # Append the row if percentage criteria met\n",
    "    \n",
    "    #Print index to keep track of progress\n",
    "    if index % 1000 == 0:\n",
    "        print(index)\n",
    "\n",
    "#%%Filter the promised text\n",
    "\n",
    "#create df\n",
    "promising_text = pd.DataFrame(promising_text)\n",
    "\n",
    "#filter out previously identified documents\n",
    "#promising_text = promising_text[~promising_text['LabelIdentifier'].isin(checked_document_labels)]\n",
    "\n",
    "#filter out duplicates (these are created with the sliding window approach)\n",
    "promising_text_unique = promising_text.drop_duplicates(subset=['LabelIdentifier'], keep='first')\n",
    "\n",
    "#save the results\n",
    "promising_text_unique.to_csv(\"output/datasets/promising_text2.csv\", index=False, sep=';')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07612ee9-b062-494e-85a4-e98919bf50c3",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "The resulting dataframe can now be qualitatively assessed and the found tables transcribed. This can then be used to georeference the locations in the tables by using the notebook matching_placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a0fa8-2326-4ac7-b8ad-64480a3894b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
