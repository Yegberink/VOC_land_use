{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbe511c-027a-467f-b61c-2e7a0e834d99",
   "metadata": {},
   "source": [
    "# Keyword-based Text Mining\n",
    "\n",
    "**In this notebook an approach to identify documents related to LU from large historical archives, with the use of keywords identified based on an earlier round of text mining. The notebook is part of a set of three notebooks which can all be found on [GitHub](https://github.com/Yegberink/VOC_land_use).**\n",
    "\n",
    "## Identify keywords\n",
    "\n",
    "The first step in this process is to identify keywords. This is done by hand, however the following code can help create some insights into what words are actually interesting by looking at the most occuring words in the interesting documents identified in the Location-Based text mining. When more interesting documents are found, this codeblock can be run again to identify additional keywords. To properly run the code, your documents should be in a similar format as datasets.xlsx. You can find the dataset on [GitHub](https://github.com/Yegberink/VOC_land_use) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14ae7a1-8af6-4a2c-a72a-5216bf2d73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas flair fuzzywuzzy python-Levenshtein bertopic ipywidgets spacy xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28073eb-28da-4525-a5e7-b76fc12f3b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import os\n",
    "from flair.data import Sentence\n",
    "from collections import Counter\n",
    "\n",
    "# Set the directory\n",
    "path = \"/Users/Yannick/Documents/LUC_github/VOC_land_use\"\n",
    "os.chdir(path)\n",
    "\n",
    "# Load document numbers of previously assessed documents\n",
    "found_datasets = pd.read_excel(\"output/results/datasets.xlsx\", sheet_name='short_summary_of_datasets')\n",
    "\n",
    "# Load the document content of the documents from Ceylon\n",
    "ceylon_documents_clean = pd.read_csv(\"Data/ceylon_documents_clean.csv\", sep=\";\")\n",
    "\n",
    "# Load stopwords\n",
    "stopwords_nl = pd.read_table('Data/stopwords_nl.txt', header=None)\n",
    "stopwords_nl = stopwords_nl[0].tolist()\n",
    "\n",
    "# initiate empty list to hold all documents that are previously assessed\n",
    "checked_document_labels = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in found_datasets.iterrows():\n",
    "    \n",
    "    #extract useful information\n",
    "    label_number = row['label_number']\n",
    "    start_page = row['start_page']\n",
    "    end_page = row['end_page']\n",
    "    \n",
    "    # Generate labels for each page in the range\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        formatted_page = str(page).zfill(4) #add 0's in front of the page number\n",
    "        label = f\"HaNA_1.04.02_{label_number}_{formatted_page}\" #fix the formatting\n",
    "        checked_document_labels.append(label) #store the labels\n",
    "\n",
    "#initiate empty list to hold all documents that were assessed to be interesting\n",
    "interesting_documents = []\n",
    "\n",
    "#filter for interesting datasets\n",
    "interesting_datasets = found_datasets[(found_datasets['interesting'] == 'yes') | (found_datasets['interesting'] == 'maybe')]\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in interesting_datasets.iterrows():\n",
    "    \n",
    "    #extract useful information\n",
    "    label_number = row['label_number']\n",
    "    start_page = row['start_page']\n",
    "    end_page = row['end_page']\n",
    "    \n",
    "    # Generate labels for each page in the range\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        formatted_page = str(page).zfill(4) #add 0's in front of the page number\n",
    "        label = f\"HaNA_1.04.02_{label_number}_{formatted_page}\" #fix the formatting\n",
    "        interesting_documents.append({\n",
    "            'label': label,\n",
    "            'interesting': row['interesting'],\n",
    "            'year': row['year'],\n",
    "            'list': row['List?'],\n",
    "            'remarks': row['remarks'],\n",
    "            'archive_number': row['label_number']}) \n",
    "\n",
    "\n",
    "interesting_documents = pd.DataFrame(interesting_documents)\n",
    "interesting_documents_labels = set(interesting_documents['label'])\n",
    "\n",
    "# create a df with interesting documents\n",
    "merged_interesting_documents = pd.merge(interesting_documents, ceylon_documents_clean, left_on='label', right_on='LabelIdentifier')\n",
    "\n",
    "# Drop the 'LabelIdentifier' column\n",
    "merged_interesting_documents = merged_interesting_documents.drop('LabelIdentifier', axis=1)\n",
    "\n",
    "# List to hold all tokens\n",
    "all_tokens = []\n",
    "texts = []\n",
    "\n",
    "# Tokenize each document using the flair Sentence tokeniser\n",
    "for text in merged_interesting_documents['DocumentContent']:\n",
    "    sentence = Sentence(text)\n",
    "    tokens = [token.text.lower() for token in sentence.tokens if len(token.text) >= 3 and token.text and token.text.lower() not in stopwords_nl]\n",
    "    all_tokens.extend(tokens)\n",
    "    texts.append(tokens)\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# Display the most common words\n",
    "most_common_words = word_counts.most_common()\n",
    "\n",
    "# Convert the result to a DataFrame for easier viewing\n",
    "most_common_words_df = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "# Save the df\n",
    "most_common_words_df.to_csv(\"output/datasets/keyword_identification.csv\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac4eee-4b61-4c5a-af82-c2f879f71ed7",
   "metadata": {},
   "source": [
    "## BERTopic\n",
    "\n",
    "In the following codeblock a BERTopic modeling is performed to check the results of the keyword identification. In this codeblock a representation of the BERTopic modeling is presented and in the codeblock after this all identified keywords for an interesting topic are extracted. The topic modeling takes some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb4a82-1aa5-4053-bd0b-81bd9f310728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "import spacy\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Extract document content from the previously loaded ceylon_documents_clean\n",
    "data = ceylon_documents_clean['DocumentContent']\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# Transform the dataframe into a list:\n",
    "data = data.apply(lambda row: ' '.join(row.values.astype(str)), axis=1).tolist()\n",
    "\n",
    "# Create the embeddings\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-V2\")\n",
    "\n",
    "# Create the dimension reduction model\n",
    "umap_model = UMAP(n_neighbors = 30, n_components = 5, min_dist = 0.0, metric = \"cosine\", random_state = 42)\n",
    "\n",
    "# Create the clustering model\n",
    "hdbscan_model = HDBSCAN(min_cluster_size = 7, metric = \"euclidean\", cluster_selection_method = 'eom', prediction_data = True)\n",
    "\n",
    "# Create the representation model\n",
    "vect_model = CountVectorizer(stop_words = stopwords_nl, min_df = 2, ngram_range = (1,1)) \n",
    "keybert_model = KeyBERTInspired()\n",
    "mmr_model = MaximalMarginalRelevance(diversity = 0.3)\n",
    "representation_model = {\n",
    "    \"keyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    " #  \"POS\": pos_model\n",
    "}\n",
    "\n",
    "# regroup all the models into one\n",
    "topic_model = BERTopic(\n",
    "    embedding_model = embed_model,\n",
    "    umap_model = umap_model,\n",
    "    hdbscan_model = hdbscan_model,\n",
    "    vectorizer_model = vect_model,\n",
    "    representation_model = representation_model,\n",
    "    top_n_words = 20, # how many words to include in the representation\n",
    "    verbose = True # this options ensures that the function returns some info while running\n",
    ")\n",
    "\n",
    "\n",
    "#Apply the model\n",
    "topics, probs = topic_model.fit_transform(data)\n",
    "\n",
    "visualisation=topic_model.visualize_hierarchy(hierarchical_topics=my_hier_topic_model)\n",
    "\n",
    "# Save the visualization as an HTML file\n",
    "visualisation.write_html(\"hierarchy_visualization.html\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341b0a9-bc94-4056-949a-f50cbc2d1100",
   "metadata": {},
   "source": [
    "## Extract keywords of interesting topics\n",
    "\n",
    "The topics in the hierarchy are analysed manually and the interesting topic numbers can be collected in the interesting_topics list. This is then used to extract the interesting keywords. Any additional interesting keywords should be added to the more_keywords document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accda7d3-6c46-4b38-896a-4a6feeaafac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of interesting topic IDs\n",
    "interesting_topics = [236, 63, 167, 285, 66, 69, 24, 129, 162, 26]\n",
    "\n",
    "# Initialize an empty list to store topic keywords\n",
    "topic_keywords = []\n",
    "\n",
    "# Loop through each interesting topic and extract keywords\n",
    "for topic in interesting_topics:\n",
    "    # Get the keywords and their respective scores for the topic\n",
    "    words, scores = zip(*topic_model.get_topic(topic))\n",
    "    \n",
    "    # Create a dictionary for the topic with words and scores\n",
    "    topic_data = {\n",
    "        \"Topic\": topic,\n",
    "        \"Keywords\": ', '.join(words),  # Join keywords into a single string for readability\n",
    "        \"Scores\": ', '.join([f'{score:.4f}' for score in scores])  # Format the scores\n",
    "    }\n",
    "    \n",
    "    # Append to the list\n",
    "    topic_keywords.append(topic_data)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_topics = pd.DataFrame(topic_keywords)\n",
    "\n",
    "# Save the results\n",
    "df_topics.to_csv(\"output/datasets/BERTopic_keywords.csv\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e7167-fc66-4d7e-80e6-003f4fe19d25",
   "metadata": {},
   "source": [
    "## Load and clean documents for keyword based text mining\n",
    "\n",
    "**The following part of the code can be used independently of the keyword identification. Therefore, some part of the previous code are repeated.**\n",
    "\n",
    "In the following codeblock packages and data are loaded and prepared for the matching of keywords with words in texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a675a06-28e3-4193-80d2-5204c2e3b9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import os\n",
    "from flair.data import Sentence\n",
    "from collections import Counter\n",
    "import string\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Set the directory\n",
    "path = \"/Users/Yannick/Documents/RA work\"\n",
    "os.chdir(path)\n",
    "\n",
    "# Load previously assessed documents\n",
    "found_datasets = pd.read_excel(\"output/results/datasets.xlsx\", sheet_name='short_summary_of_datasets')\n",
    "\n",
    "#initiate empty df\n",
    "checked_document_labels = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in found_datasets.iterrows():\n",
    "    \n",
    "    #extract useful information\n",
    "    label_number = row['label_number']\n",
    "    start_page = row['start_page']\n",
    "    end_page = row['end_page']\n",
    "    \n",
    "    # Generate labels for each page in the range\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        formatted_page = str(page).zfill(4) #add 0's in front of the page number\n",
    "        label = f\"HaNA_1.04.02_{label_number}_{formatted_page}\" #fix the formatting\n",
    "        checked_document_labels.append(label) #store the labels\n",
    "\n",
    "# Load ceylon documents\n",
    "ceylon_documents_clean = pd.read_csv(\"Data/ceylon_documents_clean\", sep=\";\")\n",
    "\n",
    "# Load the more_keywords document\n",
    "\n",
    "more_keywords = pd.read_table(\"Data/more_keywords.txt\", header=None) \n",
    "more_keywords = set(more_keywords[0])\n",
    "\n",
    "# List to hold all tokens\n",
    "all_tokens = []\n",
    "texts = []\n",
    "\n",
    "#load stopwords_nl\n",
    "stopwords_nl = pd.read_table('Data/stopwords_nl.txt', header=None)\n",
    "stopwords_nl = stopwords_nl[0].tolist()\n",
    "\n",
    "#Load trade data\n",
    "#Trade datasets\n",
    "cargo_1 = pd.read_excel(\"Data/trade_data/cargos (1).xls\", \"cargo\", header=None)\n",
    "cargo_2 = pd.read_excel(\"Data/trade_data/cargos (2).xls\", \"cargo\", header=None)\n",
    "cargo_3 = pd.read_excel(\"Data/trade_data/cargos (3).xls\", \"cargo\", header=None)\n",
    "\n",
    "#Concat into single df\n",
    "trade_data = pd.concat([cargo_1, cargo_2, cargo_3])\n",
    "\n",
    "#Select first 8 columns\n",
    "trade_data = trade_data.iloc[:,0:8]\n",
    "\n",
    "#Name these columns\n",
    "trade_data.columns = [\"book_year\", \"quantity\", \"unit\", \"product\", \"departure_place\", \"departure_region\", \"arrival_place\", \"arrival_region\"]\n",
    "\n",
    "#Convert - to NAN\n",
    "trade_data.replace('-', None, inplace=True)\n",
    "\n",
    "#Create set of unique commodities\n",
    "commodity_list = set(trade_data['product'])\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bae2a5-56a8-49dd-a1cb-37d0b95ba715",
   "metadata": {},
   "source": [
    "## Flag interesting documents\n",
    "\n",
    "In the following codeblock two keyword based filterings are applied. Firstly, only texts containing a high number of LU keywords are kept(x), and seconly only documents containing a low number of commodities are kept (y). The filtering showed no new documents when in the (x >= 2 and y < 3) or (x >= 3) configuration. The best working filtering was therefore found to be:(x >= 3 and y < 3) or (x >= 4). y was kept stable for the filtering since changing this led to the inclusion of many wrong texts. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d95c61e-814a-4db5-88ef-d3fef5569dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#%%Flag interesting documents\n",
    "\n",
    "flagged_text = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in ceylon_documents_clean.iterrows():\n",
    "    text = row['DocumentContent']  # Extract the document content\n",
    "    tokenized_text = Sentence(text)  # Tokenize the text (assuming Sentence is a tokenizer)\n",
    "    \n",
    "    #initiate counter\n",
    "    x = 0\n",
    "    y = 0\n",
    "   \n",
    "    #initiate empty lists for matched words\n",
    "    found_tokens = []\n",
    "    found_commodities = []\n",
    "    \n",
    "    # Iterate over each token in the tokenized text\n",
    "    for token in tokenized_text:\n",
    "        word = token.text.lower()  # Convert token to lowercase\n",
    "        if word in more_keywords:  # Check if token is in cultivation_keywords\n",
    "            x += 1  # Increment the counter if a keyword is found\n",
    "            found_tokens.append(word)  # Store the matching token\n",
    "        if word in commodity_list:  # Check if token is in commodity_list\n",
    "            y += 1  # Increment the counter if a commodity is found\n",
    "            found_commodities.append(word)  # Store the matching commodity\n",
    "\n",
    "    # Determine the flag and append the result to promising_text\n",
    "    if (x >= 3 and y < 3) or (x >= 4):\n",
    "        flag = 'yes'\n",
    "    else:\n",
    "        flag = 'no'\n",
    "    \n",
    "    #append the list\n",
    "    flagged_text.append({\n",
    "        'LabelIdentifier': row['LabelIdentifier'],  # Store the label identifier\n",
    "        'DocumentContent': text,  # Store the original document content\n",
    "        'tokened_text': tokenized_text,  # Store the tokenized text\n",
    "        'cultivation_words': found_tokens,  # Store the list of found cultivation keywords\n",
    "        'flag': flag,  # Store the determined flag\n",
    "        'commodities': found_commodities  # Store the list of found commodities\n",
    "    })\n",
    "    #Print index to track progress\n",
    "    if index % 10000 == 0:\n",
    "        print(index)\n",
    "\n",
    "#convert to df\n",
    "flagged_text = pd.DataFrame(flagged_text)\n",
    "\n",
    "#%%Filter for documents that have yes or maybe\n",
    "\n",
    "filtered_flagged = flagged_text[(flagged_text['flag'] == 'yes')].reset_index()\n",
    "\n",
    "#%% save the dataframe for checking\n",
    "filtered_flagged.to_csv(\"output/datasets/filtered_flagged_text.csv\", index=False, sep=';')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fef2aa-3aa7-4e6b-aa6c-80bab956fb72",
   "metadata": {},
   "source": [
    "## Table identification\n",
    "\n",
    "The last step in the identification of potentially interesting documents is the identification of tables. Due to the structure of the HTR texts from the GLOBALISED project which is used for this study, this can be done based on punctuation percentage. If the punctuation percentage is lower than 30% we found that many none-tables were returned and therefore the threshold was set on 30%. The resulting dataset is saved and can be qualitatively assessed to find pages on LU. If this assessment is done using the Excel file on [GitHub](https://github.com/Yegberink/VOC_land_use), previously assessed documents are filtered out to make the iterative process of setting the thresholds easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55c21cb9-9aa2-4efb-8aa7-898809afa901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#%% Find documents with high amounts of punctuation\n",
    "\n",
    "#Define functions\n",
    "def sliding_window(elements, window_size):\n",
    "    \"\"\"\n",
    "    Extracts sliding windows of a specified size from a list.\n",
    "    \n",
    "    Parameters:\n",
    "    elements (list): List of elements.\n",
    "    window_size (int): Size of the sliding window.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of sliding windows.\n",
    "    \"\"\"\n",
    "    if len(elements) <= window_size:\n",
    "        return [elements]  # Return the whole list as one window\n",
    "    return [elements[i:i + window_size] for i in range(len(elements) - window_size + 1)]\n",
    "\n",
    "\n",
    "def count_punctuation(sentence):\n",
    "    \"\"\"\n",
    "    Counts the number of punctuation marks in a sentence.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (str): The input sentence.\n",
    "\n",
    "    Returns:\n",
    "    int: The count of punctuation marks in the sentence.\n",
    "    \"\"\"\n",
    "    return sum(1 for char in sentence if char in string.punctuation)\n",
    "\n",
    "\n",
    "def punctuation_percentage(sentence):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of punctuation marks in a sentence. Uses the count_punctuation function.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (str): The input sentence.\n",
    "\n",
    "    Returns:\n",
    "    float: The percentage of punctuation marks in the sentence.\n",
    "    \"\"\"\n",
    "    num_punctuation = count_punctuation(sentence)\n",
    "    total_chars = len(sentence)\n",
    "    return (num_punctuation / total_chars) * 100 if total_chars > 0 else 0\n",
    "\n",
    "#do the filtering \n",
    "\n",
    "promising_text = [] #initiate list\n",
    "\n",
    "#loop ocer the filtered and flagged documents\n",
    "for index, row in filtered_flagged.iterrows():\n",
    "    text = row['tokened_text']  # Extract the document content\n",
    "    \n",
    "    #divide into parts of the document\n",
    "    for element in sliding_window(text, 30):\n",
    "        sentence = \" \".join([str(token.text) for token in element]) #create a workable string\n",
    "        percentage = punctuation_percentage(sentence) #calculate punctuation percentage\n",
    "        \n",
    "        #append the row if there is high punctuation somewhere in the document\n",
    "        if percentage >= 30:\n",
    "            promising_text.append(row)  # Append the row if percentage criteria met\n",
    "    \n",
    "    #Print index to keep track of progress\n",
    "    if index % 1000 == 0:\n",
    "        print(index)\n",
    "\n",
    "#%%Filter the promised text\n",
    "\n",
    "#create df\n",
    "promising_text = pd.DataFrame(promising_text)\n",
    "\n",
    "#filter out previously identified documents\n",
    "#promising_text = promising_text[~promising_text['LabelIdentifier'].isin(checked_document_labels)]\n",
    "\n",
    "#filter out duplicates (these are created with the sliding window approach)\n",
    "promising_text_unique = promising_text.drop_duplicates(subset=['LabelIdentifier'], keep='first')\n",
    "\n",
    "#save the results\n",
    "promising_text_unique.to_csv(\"output/datasets/promising_text2.csv\", index=False, sep=';')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07612ee9-b062-494e-85a4-e98919bf50c3",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "The resulting dataframe can now be qualitatively assessed and the found tables transcribed. This can then be used to georeference the locations in the tables by using the notebook matching_placenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a0fa8-2326-4ac7-b8ad-64480a3894b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
