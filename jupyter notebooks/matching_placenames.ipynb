{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e3c8ad-a1bd-483d-90e3-f7c1e3838d5f",
   "metadata": {},
   "source": [
    "# Matching placenames\n",
    "\n",
    "**In this notebook an automated approach is presented for matching placenames found in lists on land use to georeferenced placenames from historical maps. It provides a way to make information from historical documents spatialy explicit. The notebook is part of a set of three notebooks which can all be found on [GitHub](https://github.com/Yegberink/VOC_land_use).**\n",
    "\n",
    "\n",
    "## Load data\n",
    "\n",
    "Load the data with this approach. Update the file paths according to your data structure. To properly run the code, your documents should be in a similar format as placenames.gpkg and datasets.xlsx. You can find the datasets on [GitHub](https://github.com/Yegberink/VOC_land_use) for reference. In this codeblock also the regions are specified. In this case these are the overarching regions of which maps are found. The packages are also loaded in this codeblock. Uncomment the install if (some of) the are not yet installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38a5cb76-8b57-4b5b-95e0-bc7750a0ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install geopandas pandas fuzzywuzzy scikit-learn numpy python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97cdfaf1-e890-4a5b-b794-3f55ad2c1b16",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'fiona' has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(path)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#load the georeferenced placenames\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m placenames_maps \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaps/placenames/placenames.gpkg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Load the Excel file\u001b[39;00m\n\u001b[1;32m     19\u001b[0m xls \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mExcelFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/results/datasets.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/new_env/lib/python3.8/site-packages/geopandas/io/file.py:281\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown engine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/new_env/lib/python3.8/site-packages/geopandas/io/file.py:299\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere requires fiona 1.9+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_bytes:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# Opening a file via URL or file-like-object above automatically detects a\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# zipped file. In order to match that behavior, attempt to add a zip scheme\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# if missing.\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    300\u001b[0m         parsed \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mparse_path(\u001b[38;5;28mstr\u001b[39m(path_or_bytes))\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed, fiona\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mParsedPath):\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;66;03m# If fiona is able to parse the path, we can safely look at the scheme\u001b[39;00m\n\u001b[1;32m    303\u001b[0m             \u001b[38;5;66;03m# and update it to have a zip scheme if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/new_env/lib/python3.8/site-packages/geopandas/io/file.py:166\u001b[0m, in \u001b[0;36m_is_zip\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_zip\u001b[39m(path):\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check if a given path is a zipfile\"\"\"\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[43mfiona\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241m.\u001b[39mParsedPath\u001b[38;5;241m.\u001b[39mfrom_uri(path)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    168\u001b[0m         parsed\u001b[38;5;241m.\u001b[39marchive\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39marchive\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'fiona' has no attribute 'path'"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Set the directory\n",
    "path = \"/Users/Yannick/Documents/LUC_github/VOC_land_use\"\n",
    "os.chdir(path)\n",
    "\n",
    "#load the georeferenced placenames\n",
    "placenames_maps = gpd.read_file(\"maps/placenames/placenames.gpkg\").dropna(subset=['geometry'])\n",
    "\n",
    "# Load the Excel file\n",
    "xls = pd.ExcelFile(\"output/results/datasets.xlsx\")\n",
    "\n",
    "#Get all sheetnames from the file\n",
    "sheet_names = xls.sheet_names\n",
    "\n",
    "# Specify some unwanted sheets (update according to your data structure)\n",
    "unwanted_sheets = [\"short_summary_of_datasets\", \"template_information\"]\n",
    "\n",
    "#Filter out unwanted sheets\n",
    "filtered_sheet_names = [name for name in sheet_names if name not in unwanted_sheets]\n",
    "\n",
    "# Initialize an empty list to store each document's cleaned dataset\n",
    "all_documents = []\n",
    "\n",
    "# Loop through the remaining sheet names to load, clean, and append each dataset\n",
    "for document_number in filtered_sheet_names:\n",
    "    \n",
    "    # Load data from the current sheet\n",
    "    dataset = pd.read_excel(xls, sheet_name=document_number)\n",
    "    \n",
    "    # Remove NaN values from the 'Village' column and ensure unique village names within this dataset\n",
    "    dataset_cleaned = dataset.dropna(subset=['Village']).drop_duplicates(subset=['Village'])\n",
    "    \n",
    "    # Add a new column 'sheet_name' to track which sheet the data came from\n",
    "    dataset_cleaned['sheet_name'] = document_number\n",
    "    \n",
    "    # Append the cleaned dataset to the list\n",
    "    all_documents.append(dataset_cleaned)\n",
    "\n",
    "# Concatenate all datasets into one DataFrame\n",
    "combined_dataset = pd.concat(all_documents, ignore_index=True)\n",
    "\n",
    "# Generate sequential IDs for the grouped dataset\n",
    "combined_dataset['ID'] = range(1, len(combined_dataset) + 1)\n",
    "\n",
    "# Initiate the regions\n",
    "regions = ['happitigam corle', 'hina corle', 'alloetcoer corle']\n",
    "\n",
    "print(\"done loading data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb0e0b-206a-4501-89bc-41a3611ac887",
   "metadata": {},
   "source": [
    "## Clean transcription data\n",
    "\n",
    "For the matching procedure to work there should be only unique placenames in the transcribed tables. To ensure that this happened properly we standardise the placenames. Whenever there is a similarity score of 86 or higher the placenames are considered to be equal and the most occuring placename is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28afdf65-7d3a-4335-81c9-a3cbe2fc93fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cleaning the transcribed tables\n"
     ]
    }
   ],
   "source": [
    "# Get the frequency of each village name (case insensitive)\n",
    "village_frequencies = Counter(combined_dataset['Village'].str.lower())\n",
    "\n",
    "# Initialize an empty dictionary to store grouped village names with similar ones\n",
    "similarity_groups = {}\n",
    "\n",
    "# Loop through each unique village in the combined dataset\n",
    "for idx, row in combined_dataset.iterrows():\n",
    "    placename = row[\"Village\"].lower()\n",
    "\n",
    "    # Compare the current village name with all other village names in the combined dataset\n",
    "    for index, row2 in combined_dataset.iterrows():\n",
    "        item = row2['Village'].lower()\n",
    "\n",
    "        # Skip if comparing the same name\n",
    "        if placename == item:\n",
    "            continue\n",
    "        \n",
    "        # Calculate similarity score using fuzz.ratio\n",
    "        similarity_score = fuzz.ratio(placename, item)\n",
    "        \n",
    "        # Check if the similarity score is higher than or equal to 86\n",
    "        if similarity_score >= 86:\n",
    "            if placename not in similarity_groups:\n",
    "                similarity_groups[placename] = [placename]\n",
    "            if item not in similarity_groups[placename]:\n",
    "                similarity_groups[placename].append(item)\n",
    "\n",
    "# Function to decide the canonical name by frequency (and length in case of ties)\n",
    "def get_canonical_name_by_frequency(similar_villages, village_frequencies):\n",
    "    freq_dict = {village: village_frequencies[village] for village in similar_villages}\n",
    "    \n",
    "    # Sort by frequency (descending), and by length (ascending) if frequencies tie\n",
    "    canonical_name = sorted(freq_dict, key=lambda x: (-freq_dict[x], len(x)))[0]\n",
    "    \n",
    "    return canonical_name\n",
    "\n",
    "# Process similarity groups to choose a canonical name for each group based on frequency\n",
    "canonical_names = {}\n",
    "for group_key, similar_villages in similarity_groups.items():\n",
    "    canonical_name = get_canonical_name_by_frequency(similar_villages, village_frequencies)\n",
    "    for village in similar_villages:\n",
    "        canonical_names[village] = canonical_name  # Map all similar villages to the canonical name\n",
    "\n",
    "# Apply the canonical name replacements to the combined dataset\n",
    "for index, row in combined_dataset.iterrows():\n",
    "    village_name = row['Village'].lower()\n",
    "    if village_name in canonical_names:\n",
    "        combined_dataset.at[index, 'Village'] = canonical_names[village_name]\n",
    "\n",
    "# Remove duplicates and save the standardized combined dataset\n",
    "standardised_combined = combined_dataset.drop_duplicates()\n",
    "\n",
    "# Group by 'Village' and 'Region2' for aggregation\n",
    "dataset_cleaned = standardised_combined.groupby(['Village', 'Region2'], as_index=False).agg({\n",
    "    'ID': lambda x: list(x),  # Concatenate IDs into a list\n",
    "    'Region1': \"first\"\n",
    "})\n",
    "\n",
    "print(\"Done cleaning the transcribed tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6dd67-aaa9-4cb8-b7d2-348eb6b8591e",
   "metadata": {},
   "source": [
    "## Define the matching functions\n",
    "\n",
    "Two functions are used in the matching of placenames. The first function (clustering approach) is used to identify subregions using fuzzy matching and spatial clustering. The code loops first over overarching regions and then subregions which are specified in the historical documents. The overarching region is also specified in the maps, however the subregions are not. To each location a matching score is assigned based on the likeness in spelling (fuzzy matching) and the proximity to a regional centroid which is identified using a clustering approach. The highest matching score is then kept and the dataframe is filtered to only contain the highest matching scores. \n",
    "\n",
    "The second function (fuzzy approach) uses only fuzzy mathing to find placename matches. \n",
    "\n",
    "*Parameters clustering approach:*\n",
    "\n",
    "- Dataset: A pandas DataFrame containing placename data with columns ['Region1', 'Region2', 'Village', 'ID'].\n",
    "- PlacenamesMap: A GeoDataFrame containing placenames and geometries with columns ['region', 'placename', 'geometry'].\n",
    "- regions: A list of regions to perform matching in (corresponds to 'Region2' column in Dataset and 'region' in PlacenamesMap).\n",
    "- subregions: An optional set of subregions to filter by (corresponds to 'Region1' in Dataset).\n",
    "- min_similarity_score: Minimum fuzzy matching score to consider a match (default is 75).\n",
    "- dbscan_eps_factor: Factor of the maximum distance used to calculate the dbscan epsilon (default is 0.05).\n",
    "- dbscan_min_samples: Minimum samples for DBSCAN to form a cluster (default is 2).\n",
    "- min_matching_score: Minimum matching score for a match to be considered valid (default is 23).\n",
    "\n",
    "*Parameters fuzzy approach:*\n",
    "\n",
    "- Dataset: A pandas DataFrame containing placename data with columns ['Region1', 'Region2', 'Village', 'ID'].\n",
    "- PlacenamesMap: A GeoDataFrame containing placenames and geometries with columns ['region', 'placename', 'geometry'].\n",
    "- regions: A list of regions to perform matching in (corresponds to 'Region2' column in Dataset and 'region' in PlacenamesMap).\n",
    "- subregions: An optional set of subregions to filter by (corresponds to 'Region1' in Dataset).\n",
    "- min_similarity_score: Minimum fuzzy matching score to consider a match (default is 75)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a481bae-b03f-4f69-97a7-44124d68ecad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions defined\n"
     ]
    }
   ],
   "source": [
    "def fuzzy_approach(Dataset, PlacenamesMap, regions, min_similarity_score=75):\n",
    "    \n",
    "    # Initialize an empty DataFrame to store all the matched placenames across regions\n",
    "    matched_placenames = pd.DataFrame()\n",
    "\n",
    "    # Iterate through each region provided in the `regions` list\n",
    "    for region in regions:\n",
    "        \n",
    "        print(region)  # Print current region for progress tracking\n",
    "        \n",
    "        # Filter Dataset for rows that match the current region in 'Region2' column\n",
    "        filtered_dataset_region = Dataset[Dataset['Region2'] == region]\n",
    "        \n",
    "        # Filter PlacenamesMap for rows that match the current region in 'region' column\n",
    "        filtered_placenames_map = PlacenamesMap[PlacenamesMap['region'] == region]\n",
    "        \n",
    "        # Initialize an empty DataFrame to hold the matched placenames for this region\n",
    "        matched_placenames_region = pd.DataFrame()\n",
    "\n",
    "        regional_placenames = []\n",
    "\n",
    "        # Iterate over each row in the filtered placenames map\n",
    "        for index, row in filtered_placenames_map.iterrows():\n",
    "            placename = row['placename']  # Now, row is the actual Pandas Series\n",
    "                \n",
    "            # Iterate over each row in the filtered dataset (subregion level)\n",
    "            for index2, row2 in filtered_dataset_region.iterrows():\n",
    "                item = row2['Village']  # Extract the village name from the dataset\n",
    "                ID = row2['ID']  # Extract the ID\n",
    "                # Calculate the fuzzy similarity score between placename and the village name\n",
    "                similarity_score = fuzz.ratio(placename.lower(), item.lower())  # Use fuzzy matching\n",
    "                    \n",
    "                # Check if the similarity score meets the minimum threshold\n",
    "                if similarity_score >= min_similarity_score:\n",
    "                    matched_row = row.copy()  # Copy the placename row to modify\n",
    "                        \n",
    "                    # Add relevant fields to the matched row\n",
    "                    matched_row[\"Village\"] = item  # Matched village name\n",
    "                    matched_row[\"similarity_score\"] = similarity_score  # Add similarity score\n",
    "                    matched_row[\"ID\"] = ID  # Add the village ID\n",
    "                    matched_row[\"label\"] = f\"{item}{similarity_score}{placename}\"  # Create a label for later use\n",
    "                        \n",
    "                    # Append the matched row to the list of regional matches\n",
    "                    regional_placenames.append(matched_row)\n",
    "\n",
    "        # Create a GeoDataFrame from the matched rows\n",
    "        matched_placenames_region = gpd.GeoDataFrame(\n",
    "            regional_placenames,\n",
    "            geometry=[row['geometry'] for row in regional_placenames],  # Extract geometries for the GeoDataFrame\n",
    "            crs=\"EPSG:5234\"\n",
    "        )\n",
    "\n",
    "        # Check for multiple matches of the same village\n",
    "        if not matched_placenames_region.empty:\n",
    "            \n",
    "            #Sort the dataframe\n",
    "            matched_placenames_region = matched_placenames_region.sort_values(by='similarity_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "            # Initialize sets to track seen villages and geometries\n",
    "            seen_villages = set()\n",
    "            seen_geometries = set()\n",
    "\n",
    "            # Create a list to store the index of rows to keep\n",
    "            rows_to_keep = []\n",
    "\n",
    "            # Loop through the dataframe rows\n",
    "            for index, row in matched_placenames_region.iterrows():\n",
    "                village = row['Village']\n",
    "                geometry = row['geometry']\n",
    "                \n",
    "                # Check if the village or geometry is already seen\n",
    "                if village not in seen_villages and geometry not in seen_geometries:\n",
    "                    # If not seen, add them to the set and mark the row to keep\n",
    "                    seen_villages.add(village)\n",
    "                    seen_geometries.add(geometry)\n",
    "                    rows_to_keep.append(index)\n",
    "\n",
    "            # Filter the dataframe to keep only the best matches (the rows we marked to keep)\n",
    "            filtered_matched_placenames = matched_placenames_region.loc[rows_to_keep].reset_index(drop=True)\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            print(\"No valid matches found with non-null IDs and matching scores.\")\n",
    "            filtered_matched_placenames = pd.DataFrame()  # Return an empty DataFrame if no valid matches\n",
    "\n",
    "     \n",
    "        # Append the results for the current region to the overall matched placenames DataFrame\n",
    "        matched_placenames = pd.concat([matched_placenames, filtered_matched_placenames], ignore_index=True)\n",
    "\n",
    "    # Return the final DataFrame containing all matched placenames\n",
    "    return matched_placenames\n",
    "\n",
    "#Define the clustering approach\n",
    "def clustering_approach(Dataset, PlacenamesMap, regions, min_similarity_score=75, dbscan_eps_factor=0.05, dbscan_min_samples=2, min_matching_score=23, location_weight=1, fuzzy_match_weight=1):\n",
    "    \n",
    "    # Iterate through each region provided in the `regions` list\n",
    "    matched_placenames = pd.DataFrame()\n",
    "    for region in regions:\n",
    "        print(region)  # Print current region for progress tracking\n",
    "        \n",
    "        # Filter Dataset for rows that match the current region in 'Region2' column\n",
    "        filtered_dataset_region = Dataset[Dataset['Region2'] == region]\n",
    "        \n",
    "        # Split Dataset to account for non_specified subregion\n",
    "        filtered_dataset_subregion_specified = filtered_dataset_region[filtered_dataset_region['Region1'] != \"not_specified\"]\n",
    "        \n",
    "        # Filter PlacenamesMap for rows that match the current region in 'region' column\n",
    "        filtered_placenames_map = PlacenamesMap[PlacenamesMap['region'] == region]\n",
    "        \n",
    "        # Initialize an empty DataFrame to hold the matched placenames for this region\n",
    "        matched_placenames_region = pd.DataFrame()\n",
    "\n",
    "        # Extract unique subregions within the filtered dataset\n",
    "        subregions = set(filtered_dataset_subregion_specified['Region1'].dropna())\n",
    "        print(subregions)  # Print the subregions for tracking\n",
    "\n",
    "        # Calculate the centroid of all geometries in the filtered placenames map\n",
    "        overarching_centroid = filtered_placenames_map.union_all().centroid\n",
    "        \n",
    "        # Calculate the distance of each placename from this overarching centroid\n",
    "        filtered_placenames_map['distance_to_overarching_centroid'] = (filtered_placenames_map['geometry'].distance(overarching_centroid))\n",
    "        \n",
    "        # Get the maximum distance to the overarching centroid (used later for normalization)\n",
    "        max_distance = filtered_placenames_map['distance_to_overarching_centroid'].max()\n",
    "\n",
    "        # Iterate through each subregion to perform matching\n",
    "        for subregion in subregions:\n",
    "            \n",
    "            # Filter the dataset for the current subregion\n",
    "            filtered_dataset_subregion = filtered_dataset_subregion_specified[filtered_dataset_subregion_specified['Region1'] == subregion]\n",
    "            \n",
    "            # List to store matched placenames for the current subregion\n",
    "            regional_placenames = []\n",
    "        \n",
    "            # Iterate over each row in the filtered placenames map\n",
    "            for index, row in filtered_placenames_map.iterrows():\n",
    "                placename = row['placename']  # Now, row is the actual Pandas Series\n",
    "                \n",
    "                # Iterate over each row in the filtered dataset (subregion level)\n",
    "                for index2, row2 in filtered_dataset_subregion.iterrows():\n",
    "                    item = row2['Village']  # Extract the village name from the dataset\n",
    "                    ID = row2['ID']  # Extract the ID\n",
    "                    \n",
    "                    # Calculate the fuzzy similarity score between placename and the village name\n",
    "                    similarity_score = fuzz.ratio(placename.lower(), item.lower())  # Use fuzzy matching\n",
    "                    \n",
    "                    # Check if the similarity score meets the minimum threshold\n",
    "                    if similarity_score >= min_similarity_score:\n",
    "                        matched_row = row.copy()  # Copy the placename row to modify\n",
    "                        \n",
    "                        # Add relevant fields to the matched row\n",
    "                        matched_row[\"Village\"] = item  # Matched village name\n",
    "                        matched_row[\"similarity_score\"] = similarity_score  # Add similarity score\n",
    "                        matched_row[\"subregion\"] = subregion  # Add subregion info\n",
    "                        matched_row[\"ID\"] = ID  # Add the village ID\n",
    "                        matched_row[\"label\"] = f\"{item}\"  # Create a label for later use\n",
    "                        \n",
    "                        # Append the matched row to the list of regional matches\n",
    "                        regional_placenames.append(matched_row)\n",
    "\n",
    "            # Create a GeoDataFrame from the matched rows\n",
    "            regional_placenames_gdf = gpd.GeoDataFrame(\n",
    "                regional_placenames,\n",
    "                geometry=[row['geometry'] for row in regional_placenames],  # Extract geometries for the GeoDataFrame\n",
    "                crs=\"EPSG:5234\"\n",
    "            )\n",
    "            \n",
    "            # Convert the geometries into an array of coordinates (for DBSCAN clustering)\n",
    "            coords = np.array(list(regional_placenames_gdf.geometry.apply(lambda x: (x.x, x.y))))\n",
    "\n",
    "            # Check if there are at least 2 points (DBSCAN requires at least 2 points to form clusters)\n",
    "            if coords.shape[0] >= 2:\n",
    "                dbscan_eps = dbscan_eps_factor * max_distance  # Set eps to 5% of the max distance\n",
    "\n",
    "                # Apply DBSCAN clustering to group nearby placenames\n",
    "                db = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples).fit(coords)\n",
    "                \n",
    "                # Assign cluster labels to the GeoDataFrame\n",
    "                regional_placenames_gdf['cluster'] = db.labels_\n",
    "\n",
    "                # Count the number of unique clusters excluding noise (-1)\n",
    "                n_clusters = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)\n",
    "                print(f\"Subregion '{subregion}' has {n_clusters} cluster(s).\")\n",
    "\n",
    "                # Check if at least one cluster was formed\n",
    "                if len(set(db.labels_)) >= 1:\n",
    "                    \n",
    "                    # Identify the largest cluster by count\n",
    "                    largest_cluster = regional_placenames_gdf['cluster'].value_counts().idxmax()\n",
    "                    \n",
    "                    # Filter the points that belong to the largest cluster\n",
    "                    filtered_points = regional_placenames_gdf[regional_placenames_gdf['cluster'] == largest_cluster]\n",
    "\n",
    "                    # Calculate the centroid of the largest cluster\n",
    "                    regional_centroid = filtered_points.union_all().centroid\n",
    "\n",
    "                    # Calculate the distance of each point to the centroid of the largest cluster\n",
    "                    regional_placenames_gdf['distance_to_regional_centroid'] = (regional_placenames_gdf['geometry'].distance(regional_centroid))\n",
    "\n",
    "                    # Calculate the matching score based on distance and similarity\n",
    "                    regional_placenames_gdf['normalized_distance'] = (1 - (regional_placenames_gdf['distance_to_regional_centroid'] / max_distance))\n",
    "                    regional_placenames_gdf['matching_score'] = round(((location_weight*regional_placenames_gdf['normalized_distance']*100) + fuzzy_match_weight*regional_placenames_gdf['similarity_score'])/(fuzzy_match_weight+location_weight), 2)\n",
    "                    \n",
    "                    # Update the label with the matching score\n",
    "                    regional_placenames_gdf['label'] = regional_placenames_gdf.apply(lambda row: f\"{row['label']} {row['matching_score']}\", axis=1)\n",
    "            \n",
    "            # Handle case when only 1 match exists\n",
    "            elif len(regional_placenames_gdf) == 1:\n",
    "                regional_placenames_gdf['matching_score'] = regional_placenames_gdf['similarity_score'] * 0.7\n",
    "            \n",
    "            else:\n",
    "                print('no match found')  # If no matches, print message\n",
    "\n",
    "            # Append the matched placenames for this subregion to the regional DataFrame\n",
    "            matched_placenames_region = pd.concat([matched_placenames_region, regional_placenames_gdf], ignore_index=True)\n",
    "\n",
    "        # Check for multiple matches of the same village\n",
    "        if not matched_placenames_region.empty:\n",
    "            \n",
    "            matched_placenames_region_filtered = matched_placenames_region[matched_placenames_region['matching_score'] >= min_matching_score]\n",
    "\n",
    "            #Sort the dataframe\n",
    "            matched_placenames_region_filtered = matched_placenames_region_filtered.sort_values(by='similarity_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "            # Initialize sets to track seen villages and geometries\n",
    "            seen_villages = set()\n",
    "            seen_geometries = set()\n",
    "\n",
    "            # Create a list to store the index of rows to keep\n",
    "            rows_to_keep = []\n",
    "\n",
    "            # Loop through the dataframe rows\n",
    "            for index, row in matched_placenames_region_filtered.iterrows():\n",
    "                village = row['Village']\n",
    "                geometry = row['geometry']\n",
    "                \n",
    "                # Check if the village or geometry is already seen\n",
    "                if village not in seen_villages and geometry not in seen_geometries:\n",
    "                    # If not seen, add them to the set and mark the row to keep\n",
    "                    seen_villages.add(village)\n",
    "                    seen_geometries.add(geometry)\n",
    "                    rows_to_keep.append(index)\n",
    "\n",
    "            # Filter the dataframe to keep only the best matches (the rows we marked to keep)\n",
    "            filtered_matched_placenames = matched_placenames_region_filtered.loc[rows_to_keep].reset_index(drop=True)\n",
    "        \n",
    "        else:\n",
    "            print(\"No valid matches found with non-null IDs and matching scores.\")\n",
    "            filtered_matched_placenames = pd.DataFrame()  # Return an empty DataFrame if no valid matches\n",
    "\n",
    "\n",
    "        # Append the results for the current region to the overall matched placenames DataFrame\n",
    "        matched_placenames = pd.concat([matched_placenames, filtered_matched_placenames], ignore_index=True)\n",
    "\n",
    "    # Return the final DataFrame containing all matched placenames\n",
    "    return matched_placenames\n",
    "\n",
    "print(\"functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac4c50f-dbfb-49ee-9085-94e88ef9d0b3",
   "metadata": {},
   "source": [
    "## Apply the clustering approach\n",
    "\n",
    "In this block the previously defined clustering_approach function is applied. All parameters of the model can be updated according to your data. Some considerations for updating the parameters:\n",
    "\n",
    "- The min_similarity_score is set to 75 because a lower value resulted in many obviously wrong matches. These are not filtered out in a later stage and therefore it is adviced to keep this value high enough.\n",
    "- Because there is a min_similarity_score the min_matching_score mainly reflects the impact of the proximity to a regional centroid. This is something to keep in mind when altering it.\n",
    "-  dbscan_eps and dbscan_min_samples relate to the clustering procedure. dbscan_min_samples reflects the minimum amount of points needed to calculate a cluster, whereas the dbscan_eps reflects the distance points can be away from each other to be considered in the same cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac88a5e-a234-4230-894a-c6b7f174867d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happitigam corle\n",
      "{'gara pattoe', 'nigombo district', 'Jattiga pattoe'}\n",
      "no match found\n",
      "no match found\n",
      "hina corle\n",
      "{'oedeka pattoe', 'gangebadde pattoe', 'oedoegaha pattoe', 'adigaar pattoe', 'adicani pattoe', 'mende pattoe'}\n",
      "Subregion 'oedeka pattoe' has 1 cluster(s).\n",
      "Subregion 'gangebadde pattoe' has 0 cluster(s).\n",
      "no match found\n",
      "Subregion 'adigaar pattoe' has 0 cluster(s).\n",
      "no match found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subregion 'mende pattoe' has 2 cluster(s).\n",
      "alloetcoer corle\n",
      "{'dalia pattoe', 'ragam pattoe', 'dochegaha pattoe'}\n",
      "Subregion 'dalia pattoe' has 1 cluster(s).\n",
      "Subregion 'ragam pattoe' has 0 cluster(s).\n",
      "Subregion 'dochegaha pattoe' has 0 cluster(s).\n",
      "matching complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "# Match placenames\n",
    "georeferenced_LU_list_clustering = clustering_approach(\n",
    "    Dataset=dataset_cleaned, \n",
    "    PlacenamesMap=placenames_maps, \n",
    "    regions=regions, \n",
    "    min_similarity_score=75, \n",
    "    dbscan_eps_factor=0.1, \n",
    "    dbscan_min_samples=3, \n",
    "    min_matching_score=50,\n",
    "    location_weight=2,\n",
    "    fuzzy_match_weight=1\n",
    ")\n",
    "\n",
    "print('matching complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b467dc-d7ee-45f7-99b0-5a8214834bd3",
   "metadata": {},
   "source": [
    "## Apply the fuzzy approach\n",
    "\n",
    "In this block the previously defined fuzzy_approach function is applied. All parameters of the model can be updated according to your data. The results of the matching can be checkes relative to the subregions as found with the clustering approach. See the report for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7878e6d3-9446-486b-a9fc-ba916f84ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happitigam corle\n",
      "hina corle\n",
      "alloetcoer corle\n",
      "matching complete\n"
     ]
    }
   ],
   "source": [
    "# Match placenames\n",
    "georeferenced_LU_list_fuzzy = fuzzy_approach(\n",
    "    Dataset=dataset_cleaned, \n",
    "    PlacenamesMap=placenames_maps, \n",
    "    regions=regions, \n",
    "    min_similarity_score=75\n",
    ")\n",
    "\n",
    "print(\"matching complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1fc320-72e4-4ad8-822c-d852df9d91e9",
   "metadata": {},
   "source": [
    "## Saving the output\n",
    "\n",
    "To save the now created dataframe it is first merged with the original dataset in order to get all information stored in this. The dataframe is 'exploded' so it again contains all datapoints in the dataset not only the unique ones. After the merging a GeoPandas dataframe is created and this is split up according to the overarching regions specified earlier. The coordinate reference system (crs) should be updated to be the crs used in your project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "039ad68c-758f-43ca-9bf1-46c4fc824de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving complete\n"
     ]
    }
   ],
   "source": [
    "# Explode the 'ID' column to unpack lists into separate rows before merging\n",
    "georeferenced_LU_list_exploded_clustering = georeferenced_LU_list_clustering.explode('ID')\n",
    "georeferenced_LU_list_exploded_ = georeferenced_LU_list_fuzzy.explode('ID')\n",
    "\n",
    "# Merge dataframes\n",
    "merged_df_fuzzy = pd.merge(georeferenced_LU_list_exploded_clustering, combined_dataset.drop(columns='Village'), on=\"ID\")\n",
    "merged_df_clustering = pd.merge(georeferenced_LU_list_exploded_clustering, combined_dataset.drop(columns='Village'), on=\"ID\")\n",
    "\n",
    "\n",
    "# Create geopandas object in correct CRS\n",
    "merged_df_clustering = gpd.GeoDataFrame(\n",
    "    merged_df_clustering,\n",
    "    geometry=merged_df_clustering['geometry'],\n",
    "    crs=\"EPSG:5234\"\n",
    ")\n",
    "\n",
    "merged_df_fuzzy = gpd.GeoDataFrame(\n",
    "    merged_df_fuzzy,\n",
    "    geometry=merged_df_fuzzy['geometry'],\n",
    "    crs=\"EPSG:5234\"\n",
    ")\n",
    "\n",
    "#split the dataframe containing all matches according to the three regions and save these regions:\n",
    "# Loop through each region, filter, and save\n",
    "for region in regions:\n",
    "    # Filter the GeoDataFrame by region\n",
    "    region_gdf = merged_df_clustering[merged_df_clustering['region'] == region]\n",
    "    # Dynamically generate the save path based on region name\n",
    "    save_path_region = f'maps/placenames/clustered_placenames_{region.replace(\" \", \"_\")}.gpkg'\n",
    "\n",
    "    # Save the filtered GeoDataFrame to a new GeoPackage file\n",
    "    region_gdf.to_file(save_path_region, driver='GPKG')\n",
    "\n",
    "for region in regions:\n",
    "    # Filter the GeoDataFrame by region\n",
    "    region_gdf = merged_df_fuzzy[merged_df_fuzzy['region'] == region]\n",
    "    # Dynamically generate the save path based on region name\n",
    "    save_path_region = f'maps/placenames/fuzzy_placenames_{region.replace(\" \", \"_\")}.gpkg'\n",
    "\n",
    "    # Save the filtered GeoDataFrame to a new GeoPackage file\n",
    "    region_gdf.to_file(save_path_region, driver='GPKG')\n",
    "\n",
    "print('saving complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
